{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_small = pipeline(\"text2text-generation\", model = \"google/flan-t5-small\")\n",
    "model_base  = pipeline(\"text2text-generation\", model = \"google/flan-t5-base\")\n",
    "model_large = pipeline(\"text2text-generation\", model = \"google/flan-t5-large\")\n",
    "model_xl    = pipeline(\"text2text-generation\", model = \"google/flan-t5-xl\")\n",
    "\n",
    "models = [[\"Small\",model_small], [\"Base\", model_base], [\"Large\", model_large], [\"X-Large\", model_xl]]\n",
    "\n",
    "for name, model in models:\n",
    "    model.save_pretrained(f\"./saved_models/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee624d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "models = [[\"Small\"], [\"Base\"], [\"Large\"], [\"X-Large\"]]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i].append(pipeline(\"text2text-generation\", model = f\"./saved_models/{models[i][0]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0a3a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Small',\n",
       "  <transformers.pipelines.text2text_generation.Text2TextGenerationPipeline at 0x275147d6b10>],\n",
       " ['Base',\n",
       "  <transformers.pipelines.text2text_generation.Text2TextGenerationPipeline at 0x27514db3410>],\n",
       " ['Large',\n",
       "  <transformers.pipelines.text2text_generation.Text2TextGenerationPipeline at 0x27514d50490>],\n",
       " ['X-Large',\n",
       "  <transformers.pipelines.text2text_generation.Text2TextGenerationPipeline at 0x27514d505a0>]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66f67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Federal Reserve typically lowers interest rates as part of its monetary easing strategy. Here's how and why.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[3][1](\"When a recession begins, the Federal Reserve typically lowers interest rates as part of its monetary easing strategy. Here's how and why:\\\n",
    "What the Fed Does:\\n\\\n",
    "- Cuts the Federal Funds Rate: The Fed reduces the benchmark interest rate to make borrowing cheaper for businesses and consumers.\\n\\\n",
    "- Stimulates Economic Activity: Lower rates encourage spending and investment, which can help revive economic growth.\\n\\\n",
    "- Supports Employment: By boosting demand, the Fed aims to reduce unemployment, which often rises during recessions.\\n\\\n",
    "- Manages Inflation: If inflation is low or falling, rate cuts are more aggressive. If inflation remains high, the Fed may be more cautious.\\n\\n\\\n",
    "Historical Patterns:\\n\\\n",
    "- In past recessions (e.g., 2001, 2008, 2020), the Fed slashed rates significantly—sometimes to near zero.\\n\\\n",
    "- These moves are often accompanied by other tools like quantitative easing or forward guidance to reinforce the impact.\\n\\n\\\n",
    "Strategic Considerations:\\n\\\n",
    "- The Fed doesn’t always act immediately. It assesses inflation trends, labor market data, and financial stability before deciding.\\n\\\n",
    "- If inflation is still elevated—as seen in recent cycles—the Fed may delay or moderate rate cuts.\\n\\n\\\n",
    "Summarize the above text\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f84f3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '3 oranges cost 3 * 2 = 6 dollars. 2 apples cost 10 - 6 = 4 dollars. Each apple is 4 / 2 = 2 dollars.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[3][1](\"I bought 2 apples and three oranges for a total of 10 dollars. Each orange is 2 dollars. How much is the price of each apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  [\"How old the planet Earth is?'\",\n",
    "            \"Write a Python code to calculate sum off all elements in a list\",\n",
    "            \"Who is the current president of the USA?\",\n",
    "            \"What is a common hedging method against currency volatility?\"]\n",
    "models[3][1](prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a4be2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "threads = []\n",
    "results = [[None for _ in range(len(models))] for _ in range(len(prompts))]\n",
    "\n",
    "def run_models(i, j):\n",
    "     output = models[j][1](prompts[i])\n",
    "     results[i][j] = output[0][\"generated_text\"]\n",
    "     print(f\"Model {j} finished prompt {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e76b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 finished prompt 0\n",
      "Model 0 finished prompt 3\n",
      "Model 0 finished prompt 2\n",
      "Model 1 finished prompt 0\n",
      "Model 1 finished prompt 3\n",
      "Model 1 finished prompt 2\n",
      "Model 2 finished prompt 0\n",
      "Model 0 finished prompt 1\n",
      "Model 3 finished prompt 0\n",
      "Model 3 finished prompt 3\n",
      "Model 3 finished prompt 2\n",
      "Model 2 finished prompt 3\n",
      "Model 2 finished prompt 2\n",
      "Model 1 finished prompt 1\n",
      "Model 2 finished prompt 1\n",
      "Model 3 finished prompt 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prompts)):\n",
    "    for j in range(len(models)):\n",
    "        thread = Thread(target=run_models, args=(i,j))\n",
    "        thread.start()\n",
    "        threads.append(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae76ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1943f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tell me how old the planet Earth is?\n",
      "  Small:  10 billion years\n",
      "  Base:  4.5 billion years old\n",
      "  Large:  billions of years\n",
      "  X-Large:  4.5 billion years\n",
      "\n",
      "\n",
      "What is the Python method that returns sum off all elements in a list\n",
      "  Small:  List of elements in a list is a list of elements in a list.\n",
      "  Base:  sum = sum(list(map(int, input().split())))\n",
      "  Large:  s = sum(list(map(int,input().split())))\n",
      "  X-Large:  s = 0 for i in list(map(int, input().split())): s += i print(s)\n",
      "\n",
      "\n",
      "Who is the current president of the United States?\n",
      "  Small:  george w. bush\n",
      "  Base:  gerald ford\n",
      "  Large:  barack obama\n",
      "  X-Large:  barack obama\n",
      "\n",
      "\n",
      "What is a common hedging method against currency volatility?\n",
      "  Small:  hedging\n",
      "  Base:  hedge fund\n",
      "  Large:  hedging with a foreign currency\n",
      "  X-Large:  hedging with forward contracts\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prompts)):\n",
    "    print(f\"\\n\\n{prompts[i]}\")\n",
    "    for j in range(len(models)):\n",
    "        print(f\"  {models[j][0]}:  {results[i][j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e0605",
   "metadata": {},
   "source": [
    "## Loading a prepared T5 quantized 8-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba65829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "\n",
    "translator = ctranslate2.Translator(\"./saved_models/X-Large-8bit/\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424457d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  [\"How old the planet Earth is?'\",\n",
    "            \"Write a Python code to calculate sum off all elements in a list\",\n",
    "            \"Who is the current president of the USA?\",\n",
    "            \"What is a common hedging method against currency volatility?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce5ea8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 billion\n",
      "list = list(map(int, input().rstrip().replace(\" \"))) list.replace(\" \".join(list)) list.replace(list[1]) list.replace(list[2]) list.replace(list[3]) list.replace(list[4]) list.replace(list[5]) list.replace(list[6]) list.replace(list[7]) list.replace(list[8]) list.replace(list[9]) list.replace(list[10]) list.replace(list[11]) list.replace(list[8]) list.replace(list[9]) list.replace(list[6]) list.replace(list[7]) list.replace(list[8]) list.replace(list[9]) list.replace(list[0]) list.replace(list[1]) list.replace(list[2]) list.replace(list[0]) list.replace(list[\n",
      "barack obama\n",
      "foreign currency forward transactions\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(5):\n",
    "for prompt in prompts:\n",
    "    input_text = prompt\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "    results = translator.translate_batch([input_tokens])\n",
    "\n",
    "    output_tokens = results[0].hypotheses[0]\n",
    "    output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d44be",
   "metadata": {},
   "source": [
    "## Quantized 8-bit and 4-bit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76437dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BitsAndBytesConfig as bnb_cfg\n",
    "\n",
    "qcfg = bnb_cfg(load_in_8bit = True)\n",
    "qqcfg = bnb_cfg(load_in_4bit = True)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "qmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", quantization_config=qcfg)\n",
    "qqmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", quantization_config=qqcfg)\n",
    "\n",
    "qmodel.save_pretrained(\"qXL\")\n",
    "qqmodel.save_pretrained(\"qqXL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e144cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 29.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/X-Large\", device_map=\"auto\")\n",
    "qmodel = T5ForConditionalGeneration.from_pretrained(\"./saved_models/qXL\", device_map=\"auto\")\n",
    "qqmodel = T5ForConditionalGeneration.from_pretrained(\"./saved_models/qqXL\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291afea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I bought 2 apples and three oranges for a total of 10 dollars. Each orange is 2 dollars. How much is the price of each apple?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7487b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original XL: <pad>3 oranges cost 3 * 2 = $6. 2 apples cost 10 - 6 = $4.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "print(f\"Original XL: {tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ff41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit XL: <pad>3 oranges cost 3 * 2 = $6. 2 apples cost 10 - 6 = $4.\n"
     ]
    }
   ],
   "source": [
    "outputs = qmodel.generate(input_ids)\n",
    "print(f\"8-bit XL: {tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50388f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit XL: <pad>3 oranges cost 3 * 2 = $6. So the apples cost 10 - 6 = $4\n"
     ]
    }
   ],
   "source": [
    "outputs = qqmodel.generate(input_ids)\n",
    "print(f\"4-bit XL: {tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c397624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 4.5 billion years</s>\n",
      "<pad>s = 0 for i in range(len(list(map(int\n",
      "<pad> barack obama</s>\n",
      "<pad> hedging with forward contracts</s>\n"
     ]
    }
   ],
   "source": [
    "prompts =  [\"How old the planet Earth is?'\",\n",
    "            \"Write a Python code to calculate sum off all elements in a list\",\n",
    "            \"Who is the current president of the USA?\",\n",
    "            \"What is a common hedging method against currency volatility?\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_text = prompt\n",
    "    input_tokens = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_tokens)\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
